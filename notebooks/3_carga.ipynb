{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c980e46d",
   "metadata": {},
   "source": [
    "# üíæ Carga de Dados - Seguran√ßa P√∫blica SP\n",
    "\n",
    "Este notebook realiza a **carga** dos dados processados em destinos finais (arquivos consolidados e bancos de dados).\n",
    "\n",
    "## Objetivos:\n",
    "- Carregar dados processados de `data/processed/`\n",
    "- Consolidar dados de m√∫ltiplos anos em um √∫nico arquivo\n",
    "- Salvar em formato Parquet otimizado\n",
    "- Carregar em banco de dados (DuckDB/SQLite)\n",
    "- Criar relat√≥rios de resumo\n",
    "- Validar integridade dos dados carregados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644cff2a",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas e Fun√ß√µes de Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necess√°rias\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Adicionar o diret√≥rio src ao path\n",
    "sys.path.append(str(Path().resolve().parent / 'src'))\n",
    "\n",
    "# Importar fun√ß√µes de carga\n",
    "from load import (\n",
    "    save_to_csv,\n",
    "    save_to_parquet,\n",
    "    save_to_excel,\n",
    "    save_to_database,\n",
    "    save_metadata,\n",
    "    create_summary_report\n",
    ")\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas e fun√ß√µes de carga importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de2232",
   "metadata": {},
   "source": [
    "## 2. Configurar Diret√≥rios e Par√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50164ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diret√≥rios do projeto\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "DATA_FINAL_DIR = PROJECT_ROOT / 'data' / 'final'\n",
    "DATABASE_DIR = PROJECT_ROOT / 'data' / 'database'\n",
    "\n",
    "# Criar diret√≥rios se n√£o existirem\n",
    "DATA_FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATABASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio de dados processados: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"üìÅ Diret√≥rio de dados finais: {DATA_FINAL_DIR}\")\n",
    "print(f\"üìÅ Diret√≥rio de banco de dados: {DATABASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d815b4a",
   "metadata": {},
   "source": [
    "## 3. Carregar Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a218f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar arquivos processados dispon√≠veis\n",
    "print(\"üìÇ Arquivos dispon√≠veis em data/processed/:\\n\")\n",
    "\n",
    "csv_files = list(DATA_PROCESSED_DIR.glob('dados_processados_*.csv'))\n",
    "parquet_files = list(DATA_PROCESSED_DIR.glob('dados_processados_*.parquet'))\n",
    "\n",
    "print(f\"üìä Arquivos CSV: {len(csv_files)}\")\n",
    "for file in csv_files:\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  - {file.name} ({size_kb:.2f} KB)\")\n",
    "\n",
    "print(f\"\\nüìä Arquivos Parquet: {len(parquet_files)}\")\n",
    "for file in parquet_files:\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  - {file.name} ({size_kb:.2f} KB)\")\n",
    "\n",
    "# Carregar dados processados (preferir Parquet se dispon√≠vel)\n",
    "if parquet_files:\n",
    "    arquivo_carga = parquet_files[0]\n",
    "    df_processed = pd.read_parquet(arquivo_carga)\n",
    "    print(f\"\\n‚úÖ Dados carregados de: {arquivo_carga.name}\")\n",
    "elif csv_files:\n",
    "    arquivo_carga = csv_files[0]\n",
    "    df_processed = pd.read_csv(arquivo_carga, encoding='utf-8-sig')\n",
    "    print(f\"\\n‚úÖ Dados carregados de: {arquivo_carga.name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Nenhum arquivo processado encontrado!\")\n",
    "    print(\"Execute primeiro o notebook '2_transformacao.ipynb'\")\n",
    "    df_processed = None\n",
    "\n",
    "if df_processed is not None:\n",
    "    print(f\"üìä Dimens√µes: {df_processed.shape[0]:,} linhas x {df_processed.shape[1]} colunas\")\n",
    "    print(f\"\\nüìã Primeiras linhas:\")\n",
    "    display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8140e",
   "metadata": {},
   "source": [
    "## 4. Consolidar Dados de M√∫ltiplos Anos\n",
    "\n",
    "Simulando a consolida√ß√£o de dados de diferentes anos/per√≠odos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidar_multiplos_anos(data_dir: Path, pattern: str = 'dados_processados_*.parquet'):\n",
    "    \"\"\"\n",
    "    Consolida arquivos de m√∫ltiplos anos em um √∫nico DataFrame\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Diret√≥rio contendo os arquivos\n",
    "        pattern: Padr√£o de nome dos arquivos a consolidar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame consolidado\n",
    "    \"\"\"\n",
    "    arquivos = list(data_dir.glob(pattern))\n",
    "    \n",
    "    if not arquivos:\n",
    "        # Se n√£o encontrar Parquet, tentar CSV\n",
    "        pattern = pattern.replace('.parquet', '.csv')\n",
    "        arquivos = list(data_dir.glob(pattern))\n",
    "    \n",
    "    if not arquivos:\n",
    "        logger.warning(f\"Nenhum arquivo encontrado com o padr√£o: {pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÇ Consolidando {len(arquivos)} arquivo(s)...\\n\")\n",
    "    \n",
    "    dataframes = []\n",
    "    for arquivo in arquivos:\n",
    "        print(f\"  üìÑ Carregando: {arquivo.name}\")\n",
    "        \n",
    "        if arquivo.suffix == '.parquet':\n",
    "            df = pd.read_parquet(arquivo)\n",
    "        else:\n",
    "            df = pd.read_csv(arquivo, encoding='utf-8-sig')\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"     ‚Üí {len(df):,} registros\")\n",
    "    \n",
    "    # Concatenar todos os DataFrames\n",
    "    df_consolidado = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Consolida√ß√£o conclu√≠da!\")\n",
    "    print(f\"üìä Total de registros consolidados: {len(df_consolidado):,}\")\n",
    "    \n",
    "    return df_consolidado\n",
    "\n",
    "# Executar consolida√ß√£o\n",
    "df_consolidado = consolidar_multiplos_anos(DATA_PROCESSED_DIR)\n",
    "\n",
    "if df_consolidado is not None:\n",
    "    # Remover duplicatas que possam existir ap√≥s consolida√ß√£o\n",
    "    registros_antes = len(df_consolidado)\n",
    "    df_consolidado = df_consolidado.drop_duplicates()\n",
    "    registros_depois = len(df_consolidado)\n",
    "    \n",
    "    if registros_antes > registros_depois:\n",
    "        print(f\"\\nüóëÔ∏è  Removidas {registros_antes - registros_depois:,} duplicatas ap√≥s consolida√ß√£o\")\n",
    "    \n",
    "    print(f\"\\nüìã Estrutura dos dados consolidados:\")\n",
    "    print(df_consolidado.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67866ebb",
   "metadata": {},
   "source": [
    "## 5. Salvar em Arquivo Parquet Consolidado\n",
    "\n",
    "Usando a fun√ß√£o `save_to_parquet()` para salvar o arquivo final otimizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_consolidado is not None:\n",
    "    # Nome do arquivo consolidado\n",
    "    arquivo_final = DATA_FINAL_DIR / 'seguranca_publica_sp_consolidado.parquet'\n",
    "    \n",
    "    # Salvar usando a fun√ß√£o save_to_parquet\n",
    "    print(\"üíæ Salvando arquivo Parquet consolidado...\")\n",
    "    sucesso = save_to_parquet(\n",
    "        df_consolidado, \n",
    "        str(arquivo_final),\n",
    "        compression='snappy'  # Compress√£o eficiente\n",
    "    )\n",
    "    \n",
    "    if sucesso:\n",
    "        # Verificar tamanho do arquivo\n",
    "        size_mb = arquivo_final.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n‚úÖ Arquivo Parquet salvo com sucesso!\")\n",
    "        print(f\"üìÅ Caminho: {arquivo_final}\")\n",
    "        print(f\"üíæ Tamanho: {size_mb:.2f} MB\")\n",
    "        print(f\"üìä Registros: {len(df_consolidado):,}\")\n",
    "        print(f\"üìã Colunas: {len(df_consolidado.columns)}\")\n",
    "        \n",
    "        # Comparar com CSV (para demonstrar efici√™ncia do Parquet)\n",
    "        arquivo_csv_temp = DATA_FINAL_DIR / 'temp_comparison.csv'\n",
    "        df_consolidado.to_csv(arquivo_csv_temp, index=False, encoding='utf-8-sig')\n",
    "        size_csv_mb = arquivo_csv_temp.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\nüìä Compara√ß√£o de Tamanho:\")\n",
    "        print(f\"  CSV:     {size_csv_mb:.2f} MB\")\n",
    "        print(f\"  Parquet: {size_mb:.2f} MB\")\n",
    "        print(f\"  Economia: {((size_csv_mb - size_mb) / size_csv_mb * 100):.1f}%\")\n",
    "        \n",
    "        # Remover arquivo tempor√°rio\n",
    "        arquivo_csv_temp.unlink()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado para salvar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1900394",
   "metadata": {},
   "source": [
    "## 6. Carregar em Banco de Dados SQLite\n",
    "\n",
    "Usando a fun√ß√£o `save_to_database()` para criar um banco SQLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3623a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_consolidado is not None:\n",
    "    # Configurar conex√£o SQLite\n",
    "    db_path = DATABASE_DIR / 'seguranca_publica_sp.db'\n",
    "    connection_string = f'sqlite:///{db_path}'\n",
    "    \n",
    "    print(\"üóÑÔ∏è  Carregando dados no SQLite...\")\n",
    "    print(f\"üìÅ Banco de dados: {db_path}\\n\")\n",
    "    \n",
    "    # Salvar tabela principal\n",
    "    sucesso = save_to_database(\n",
    "        df_consolidado,\n",
    "        table_name='ocorrencias_criminais',\n",
    "        connection_string=connection_string,\n",
    "        if_exists='replace'  # Substituir se j√° existir\n",
    "    )\n",
    "    \n",
    "    if sucesso:\n",
    "        print(f\"\\n‚úÖ Dados carregados na tabela 'ocorrencias_criminais'\")\n",
    "        print(f\"üíæ Tamanho do banco: {db_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "        \n",
    "        # Criar tabelas agregadas adicionais\n",
    "        print(\"\\nüìä Criando tabelas agregadas...\")\n",
    "        \n",
    "        # Tabela agregada por munic√≠pio\n",
    "        df_por_municipio = df_consolidado.groupby('municipio').agg({\n",
    "            'ocorrencias': 'sum',\n",
    "            'vitimas': 'sum',\n",
    "            'taxa_criminalidade': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        save_to_database(\n",
    "            df_por_municipio,\n",
    "            table_name='agregado_por_municipio',\n",
    "            connection_string=connection_string,\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        # Tabela agregada por tipo de crime\n",
    "        df_por_crime = df_consolidado.groupby('tipo_crime').agg({\n",
    "            'ocorrencias': 'sum',\n",
    "            'vitimas': 'sum'\n",
    "        }).reset_index().sort_values('ocorrencias', ascending=False)\n",
    "        \n",
    "        save_to_database(\n",
    "            df_por_crime,\n",
    "            table_name='agregado_por_tipo_crime',\n",
    "            connection_string=connection_string,\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Tabelas agregadas criadas com sucesso!\")\n",
    "        print(\"   - ocorrencias_criminais (principal)\")\n",
    "        print(\"   - agregado_por_municipio\")\n",
    "        print(\"   - agregado_por_tipo_crime\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado para carregar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb4202",
   "metadata": {},
   "source": [
    "## 7. Carregar em DuckDB (Alternativa Otimizada)\n",
    "\n",
    "DuckDB √© mais eficiente para an√°lises de dados em larga escala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import duckdb\n",
    "    \n",
    "    if df_consolidado is not None:\n",
    "        # Criar banco DuckDB\n",
    "        duckdb_path = DATABASE_DIR / 'seguranca_publica_sp.duckdb'\n",
    "        \n",
    "        print(\"ü¶Ü Carregando dados no DuckDB...\")\n",
    "        print(f\"üìÅ Banco de dados: {duckdb_path}\\n\")\n",
    "        \n",
    "        # Conectar ao DuckDB\n",
    "        conn = duckdb.connect(str(duckdb_path))\n",
    "        \n",
    "        # Criar tabela a partir do DataFrame\n",
    "        conn.execute(\"DROP TABLE IF EXISTS ocorrencias_criminais\")\n",
    "        conn.execute(\"CREATE TABLE ocorrencias_criminais AS SELECT * FROM df_consolidado\")\n",
    "        \n",
    "        # Verificar cria√ß√£o\n",
    "        total_registros = conn.execute(\"SELECT COUNT(*) FROM ocorrencias_criminais\").fetchone()[0]\n",
    "        print(f\"‚úÖ Tabela 'ocorrencias_criminais' criada com {total_registros:,} registros\")\n",
    "        \n",
    "        # Criar √≠ndices para melhor performance\n",
    "        print(\"\\nüìë Criando √≠ndices...\")\n",
    "        conn.execute(\"CREATE INDEX idx_municipio ON ocorrencias_criminais(municipio)\")\n",
    "        conn.execute(\"CREATE INDEX idx_tipo_crime ON ocorrencias_criminais(tipo_crime)\")\n",
    "        conn.execute(\"CREATE INDEX idx_ano_mes ON ocorrencias_criminais(ano, mes)\")\n",
    "        print(\"‚úÖ √çndices criados\")\n",
    "        \n",
    "        # Exemplo de query anal√≠tica r√°pida\n",
    "        print(\"\\nüìä Teste de Performance - Top 5 Munic√≠pios:\")\n",
    "        resultado = conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                municipio,\n",
    "                SUM(ocorrencias) as total_ocorrencias,\n",
    "                ROUND(AVG(taxa_criminalidade), 2) as taxa_media\n",
    "            FROM ocorrencias_criminais\n",
    "            GROUP BY municipio\n",
    "            ORDER BY total_ocorrencias DESC\n",
    "            LIMIT 5\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        print(resultado)\n",
    "        \n",
    "        # Tamanho do banco\n",
    "        print(f\"\\nüíæ Tamanho do banco DuckDB: {duckdb_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(\"\\n‚úÖ DuckDB configurado com sucesso!\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è DuckDB n√£o est√° instalado.\")\n",
    "    print(\"Para instalar: pip install duckdb\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao configurar DuckDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690db99",
   "metadata": {},
   "source": [
    "## 8. Validar Dados Carregados\n",
    "\n",
    "Verificar integridade dos dados ap√≥s a carga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31127ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_consolidado is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç VALIDA√á√ÉO DOS DADOS CARREGADOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Verificar arquivo Parquet\n",
    "    arquivo_parquet = DATA_FINAL_DIR / 'seguranca_publica_sp_consolidado.parquet'\n",
    "    if arquivo_parquet.exists():\n",
    "        df_parquet_test = pd.read_parquet(arquivo_parquet)\n",
    "        print(f\"\\n‚úÖ Arquivo Parquet:\")\n",
    "        print(f\"   üìä Registros: {len(df_parquet_test):,}\")\n",
    "        print(f\"   üìã Colunas: {len(df_parquet_test.columns)}\")\n",
    "        print(f\"   üíæ Tamanho: {arquivo_parquet.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # 2. Verificar banco SQLite\n",
    "    db_sqlite = DATABASE_DIR / 'seguranca_publica_sp.db'\n",
    "    if db_sqlite.exists():\n",
    "        from sqlalchemy import create_engine\n",
    "        engine = create_engine(f'sqlite:///{db_sqlite}')\n",
    "        \n",
    "        # Contar registros\n",
    "        query = \"SELECT COUNT(*) as total FROM ocorrencias_criminais\"\n",
    "        resultado = pd.read_sql(query, engine)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Banco SQLite:\")\n",
    "        print(f\"   üìä Registros: {resultado['total'].iloc[0]:,}\")\n",
    "        print(f\"   üíæ Tamanho: {db_sqlite.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # Verificar tabelas\n",
    "        query_tabelas = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "        tabelas = pd.read_sql(query_tabelas, engine)\n",
    "        print(f\"   üìë Tabelas: {', '.join(tabelas['name'].tolist())}\")\n",
    "    \n",
    "    # 3. Verificar banco DuckDB\n",
    "    db_duckdb = DATABASE_DIR / 'seguranca_publica_sp.duckdb'\n",
    "    if db_duckdb.exists():\n",
    "        try:\n",
    "            import duckdb\n",
    "            conn = duckdb.connect(str(db_duckdb))\n",
    "            total = conn.execute(\"SELECT COUNT(*) FROM ocorrencias_criminais\").fetchone()[0]\n",
    "            \n",
    "            print(f\"\\n‚úÖ Banco DuckDB:\")\n",
    "            print(f\"   üìä Registros: {total:,}\")\n",
    "            print(f\"   üíæ Tamanho: {db_duckdb.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            \n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 4. Compara√ß√£o de integridade\n",
    "    print(f\"\\nüìä Compara√ß√£o de Integridade:\")\n",
    "    print(f\"   DataFrame Original: {len(df_consolidado):,} registros\")\n",
    "    if arquivo_parquet.exists():\n",
    "        print(f\"   Arquivo Parquet:    {len(df_parquet_test):,} registros\")\n",
    "        if len(df_consolidado) == len(df_parquet_test):\n",
    "            print(\"   ‚úÖ Integridade OK - Parquet\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado para validar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30965cb",
   "metadata": {},
   "source": [
    "## 9. Criar Relat√≥rio de Resumo\n",
    "\n",
    "Usando a fun√ß√£o `create_summary_report()` para documentar a carga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95286e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_consolidado is not None:\n",
    "    # Adicionar coluna 'data' para o relat√≥rio (se n√£o existir)\n",
    "    if 'data' not in df_consolidado.columns and 'ano' in df_consolidado.columns and 'mes' in df_consolidado.columns:\n",
    "        df_consolidado['data'] = pd.to_datetime(\n",
    "            df_consolidado['ano'].astype(str) + '-' + \n",
    "            df_consolidado['mes'].astype(str).str.zfill(2) + '-01'\n",
    "        )\n",
    "    \n",
    "    # Criar relat√≥rio usando a fun√ß√£o\n",
    "    print(\"üìù Gerando relat√≥rio de resumo...\")\n",
    "    sucesso = create_summary_report(df_consolidado, str(DATA_FINAL_DIR))\n",
    "    \n",
    "    if sucesso:\n",
    "        # Ler e exibir o relat√≥rio\n",
    "        summary_path = DATA_FINAL_DIR / 'summary_report.json'\n",
    "        with open(summary_path, 'r', encoding='utf-8') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(\"\\n‚úÖ Relat√≥rio de resumo criado!\")\n",
    "        print(f\"üìÅ Caminho: {summary_path}\")\n",
    "        print(\"\\nüìã Conte√∫do do Relat√≥rio:\")\n",
    "        print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado para gerar relat√≥rio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba98ff",
   "metadata": {},
   "source": [
    "## 10. Salvar Metadados da Carga\n",
    "\n",
    "Usando a fun√ß√£o `save_metadata()` para documentar o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_consolidado is not None:\n",
    "    # Criar metadados da carga\n",
    "    metadata_carga = {\n",
    "        'data_carga': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'notebook': '3_carga.ipynb',\n",
    "        'etapa': 'CARGA - Consolida√ß√£o e Armazenamento',\n",
    "        'dados_consolidados': {\n",
    "            'total_registros': int(len(df_consolidado)),\n",
    "            'total_colunas': int(len(df_consolidado.columns)),\n",
    "            'colunas': list(df_consolidado.columns),\n",
    "            'periodo': {\n",
    "                'ano_inicio': int(df_consolidado['ano'].min()),\n",
    "                'ano_fim': int(df_consolidado['ano'].max()),\n",
    "                'mes_inicio': int(df_consolidado['mes'].min()),\n",
    "                'mes_fim': int(df_consolidado['mes'].max())\n",
    "            }\n",
    "        },\n",
    "        'arquivos_gerados': {\n",
    "            'parquet': 'seguranca_publica_sp_consolidado.parquet',\n",
    "            'banco_sqlite': 'seguranca_publica_sp.db',\n",
    "            'banco_duckdb': 'seguranca_publica_sp.duckdb'\n",
    "        },\n",
    "        'destinos': [\n",
    "            'Arquivo Parquet consolidado (data/final/)',\n",
    "            'Banco de dados SQLite (data/database/)',\n",
    "            'Banco de dados DuckDB (data/database/)'\n",
    "        ],\n",
    "        'tabelas_criadas': {\n",
    "            'sqlite': [\n",
    "                'ocorrencias_criminais',\n",
    "                'agregado_por_municipio',\n",
    "                'agregado_por_tipo_crime'\n",
    "            ],\n",
    "            'duckdb': [\n",
    "                'ocorrencias_criminais'\n",
    "            ]\n",
    "        },\n",
    "        'estatisticas': {\n",
    "            'total_ocorrencias': int(df_consolidado['ocorrencias'].sum()),\n",
    "            'total_vitimas': int(df_consolidado['vitimas'].sum()),\n",
    "            'taxa_media_criminalidade': float(df_consolidado['taxa_criminalidade'].mean()),\n",
    "            'municipios': int(df_consolidado['municipio'].nunique()),\n",
    "            'tipos_crime': int(df_consolidado['tipo_crime'].nunique())\n",
    "        },\n",
    "        'qualidade': {\n",
    "            'valores_nulos': int(df_consolidado.isnull().sum().sum()),\n",
    "            'duplicatas': int(df_consolidado.duplicated().sum()),\n",
    "            'integridade': 'OK'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Salvar metadados usando a fun√ß√£o\n",
    "    metadata_path = DATA_FINAL_DIR / 'metadata_carga.json'\n",
    "    sucesso = save_metadata(metadata_carga, str(metadata_path))\n",
    "    \n",
    "    if sucesso:\n",
    "        print(\"‚úÖ Metadados da carga salvos com sucesso!\")\n",
    "        print(f\"üìÅ Caminho: {metadata_path}\")\n",
    "        print(f\"\\nüìã Resumo dos Metadados:\")\n",
    "        print(json.dumps(metadata_carga, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado para gerar metadados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98869ce",
   "metadata": {},
   "source": [
    "## 11. Exemplos de Consultas nos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda27027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Consulta no SQLite\n",
    "db_sqlite = DATABASE_DIR / 'seguranca_publica_sp.db'\n",
    "\n",
    "if db_sqlite.exists():\n",
    "    from sqlalchemy import create_engine\n",
    "    engine = create_engine(f'sqlite:///{db_sqlite}')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç EXEMPLOS DE CONSULTAS SQL - SQLite\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Query 1: Top 10 munic√≠pios com mais ocorr√™ncias\n",
    "    print(\"\\n1Ô∏è‚É£ Top 10 Munic√≠pios com Mais Ocorr√™ncias:\")\n",
    "    query1 = \"\"\"\n",
    "        SELECT \n",
    "            municipio,\n",
    "            SUM(ocorrencias) as total_ocorrencias,\n",
    "            ROUND(AVG(taxa_criminalidade), 2) as taxa_media\n",
    "        FROM ocorrencias_criminais\n",
    "        GROUP BY municipio\n",
    "        ORDER BY total_ocorrencias DESC\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    resultado1 = pd.read_sql(query1, engine)\n",
    "    print(resultado1.to_string(index=False))\n",
    "    \n",
    "    # Query 2: Evolu√ß√£o mensal de crimes\n",
    "    print(\"\\n\\n2Ô∏è‚É£ Evolu√ß√£o Mensal de Crimes:\")\n",
    "    query2 = \"\"\"\n",
    "        SELECT \n",
    "            ano,\n",
    "            mes,\n",
    "            SUM(ocorrencias) as total_ocorrencias\n",
    "        FROM ocorrencias_criminais\n",
    "        GROUP BY ano, mes\n",
    "        ORDER BY ano, mes\n",
    "    \"\"\"\n",
    "    resultado2 = pd.read_sql(query2, engine)\n",
    "    print(resultado2.to_string(index=False))\n",
    "    \n",
    "    # Query 3: Crimes por categoria\n",
    "    print(\"\\n\\n3Ô∏è‚É£ Total de Ocorr√™ncias por Categoria:\")\n",
    "    query3 = \"\"\"\n",
    "        SELECT \n",
    "            categoria_crime,\n",
    "            COUNT(*) as quantidade_registros,\n",
    "            SUM(ocorrencias) as total_ocorrencias,\n",
    "            SUM(vitimas) as total_vitimas\n",
    "        FROM ocorrencias_criminais\n",
    "        GROUP BY categoria_crime\n",
    "        ORDER BY total_ocorrencias DESC\n",
    "    \"\"\"\n",
    "    resultado3 = pd.read_sql(query3, engine)\n",
    "    print(resultado3.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Banco SQLite n√£o encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb712c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Consulta no DuckDB (mais r√°pido para an√°lises)\n",
    "db_duckdb = DATABASE_DIR / 'seguranca_publica_sp.duckdb'\n",
    "\n",
    "if db_duckdb.exists():\n",
    "    try:\n",
    "        import duckdb\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"ü¶Ü EXEMPLOS DE CONSULTAS SQL - DuckDB\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        conn = duckdb.connect(str(db_duckdb))\n",
    "        \n",
    "        # Query 1: An√°lise agregada complexa\n",
    "        print(\"\\n1Ô∏è‚É£ An√°lise Comparativa por Munic√≠pio (Top 5):\")\n",
    "        query_duckdb = \"\"\"\n",
    "            SELECT \n",
    "                municipio,\n",
    "                COUNT(DISTINCT tipo_crime) as tipos_crime_distintos,\n",
    "                SUM(ocorrencias) as total_ocorrencias,\n",
    "                ROUND(AVG(taxa_criminalidade), 2) as taxa_media,\n",
    "                MAX(ocorrencias) as max_ocorrencias_mes\n",
    "            FROM ocorrencias_criminais\n",
    "            GROUP BY municipio\n",
    "            ORDER BY total_ocorrencias DESC\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "        resultado_duckdb = conn.execute(query_duckdb).fetchdf()\n",
    "        print(resultado_duckdb.to_string(index=False))\n",
    "        \n",
    "        # Query 2: Tend√™ncia temporal\n",
    "        print(\"\\n\\n2Ô∏è‚É£ Tend√™ncia de Crimes Violentos ao Longo do Tempo:\")\n",
    "        query_tendencia = \"\"\"\n",
    "            SELECT \n",
    "                ano,\n",
    "                mes,\n",
    "                categoria_crime,\n",
    "                SUM(ocorrencias) as total\n",
    "            FROM ocorrencias_criminais\n",
    "            WHERE categoria_crime = 'Crimes Violentos'\n",
    "            GROUP BY ano, mes, categoria_crime\n",
    "            ORDER BY ano, mes\n",
    "        \"\"\"\n",
    "        resultado_tendencia = conn.execute(query_tendencia).fetchdf()\n",
    "        print(resultado_tendencia.to_string(index=False))\n",
    "        \n",
    "        conn.close()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è DuckDB n√£o est√° instalado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Banco DuckDB n√£o encontrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e387e95",
   "metadata": {},
   "source": [
    "## 12. Resumo Final e Pr√≥ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42231fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo completo do processo de carga\n",
    "print(\"=\"*70)\n",
    "print(\"üìä RESUMO COMPLETO DO PROCESSO DE CARGA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ ETAPAS CONCLU√çDAS:\")\n",
    "print(\"  1. ‚úì Carregamento de dados processados\")\n",
    "print(\"  2. ‚úì Consolida√ß√£o de m√∫ltiplos anos/per√≠odos\")\n",
    "print(\"  3. ‚úì Salvamento em arquivo Parquet otimizado\")\n",
    "print(\"  4. ‚úì Carga em banco de dados SQLite\")\n",
    "print(\"  5. ‚úì Carga em banco de dados DuckDB\")\n",
    "print(\"  6. ‚úì Valida√ß√£o de integridade dos dados\")\n",
    "print(\"  7. ‚úì Cria√ß√£o de relat√≥rios e metadados\")\n",
    "print(\"  8. ‚úì Exemplos de consultas SQL\")\n",
    "\n",
    "print(\"\\nüìÅ ARQUIVOS E BANCOS CRIADOS:\")\n",
    "\n",
    "# Listar arquivos em data/final\n",
    "print(\"\\n  üìÇ data/final/:\")\n",
    "if DATA_FINAL_DIR.exists():\n",
    "    for file in sorted(DATA_FINAL_DIR.glob('*')):\n",
    "        size = file.stat().st_size\n",
    "        if size > 1024*1024:\n",
    "            size_str = f\"{size/(1024*1024):.2f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size/1024:.2f} KB\"\n",
    "        print(f\"    üìÑ {file.name:<45} ({size_str})\")\n",
    "\n",
    "# Listar bancos em data/database\n",
    "print(\"\\n  üìÇ data/database/:\")\n",
    "if DATABASE_DIR.exists():\n",
    "    for file in sorted(DATABASE_DIR.glob('*')):\n",
    "        size = file.stat().st_size\n",
    "        if size > 1024*1024:\n",
    "            size_str = f\"{size/(1024*1024):.2f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size/1024:.2f} KB\"\n",
    "        print(f\"    üóÑÔ∏è  {file.name:<45} ({size_str})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PROCESSO DE CARGA CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüéØ DADOS PRONTOS PARA:\")\n",
    "print(\"  ‚Ä¢ An√°lises explorat√≥rias avan√ßadas\")\n",
    "print(\"  ‚Ä¢ Cria√ß√£o de dashboards e visualiza√ß√µes\")\n",
    "print(\"  ‚Ä¢ Machine Learning e modelagem preditiva\")\n",
    "print(\"  ‚Ä¢ Integra√ß√£o com ferramentas de BI (Power BI, Tableau, etc.)\")\n",
    "print(\"  ‚Ä¢ APIs e aplica√ß√µes web\")\n",
    "\n",
    "print(\"\\nüìö COMO ACESSAR OS DADOS:\")\n",
    "print(\"  ‚Ä¢ Parquet: pd.read_parquet('data/final/seguranca_publica_sp_consolidado.parquet')\")\n",
    "print(\"  ‚Ä¢ SQLite:  pd.read_sql('SELECT * FROM ocorrencias_criminais', engine)\")\n",
    "print(\"  ‚Ä¢ DuckDB:  conn.execute('SELECT * FROM ocorrencias_criminais').fetchdf()\")\n",
    "\n",
    "print(\"\\nüöÄ PR√ìXIMO PASSO:\")\n",
    "print(\"  Execute o notebook '4_analise_dados.ipynb' para an√°lises explorat√≥rias!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
